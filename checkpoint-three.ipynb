{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "26037d32-2047-4157-81ef-595916bd66a0"
            },
            "source": [
                "# Checkpoint Three: Cleaning Data\n",
                "\n",
                "Now you are ready to clean your data. Before starting coding, provide the link to your dataset below.\n",
                "\n",
                "My dataset:\n",
                "\n",
                "Import the necessary libraries and create your dataframe(s)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "azdata_cell_guid": "e8adef8e-d0f2-4640-a179-5997f11e82ca"
            },
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
                        "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
                        "\u001b[1;31mCommand: '\"c:/Users/Office Admin/AppData/Local/Microsoft/WindowsApps/python3.12.exe\" -m pip install ipykernel -U --user --force-reinstall'"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import matplotlib as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "e172475a-c4ee-414a-8367-9965355dbba6"
            },
            "source": [
                "## Missing Data\n",
                "\n",
                "Test your dataset for missing data and handle it as needed. Make notes in the form of code comments as to your thought process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "e1dc66ef-e471-4c27-92e7-ee878c106eba"
            },
            "outputs": [],
            "source": [
                "movies_total_gross = pd.read_csv('disney_movies_total_gross.csv')\n",
                "disney_revenue = pd.read_csv('disney_revenue_1991-2016.csv')\n",
                "characters = pd.read_csv('Disney_Characters_cleaned.csv')\n",
                "\n",
                "disney_main_df = pd.concat([movies_total_gross,disney_revenue,characters],ignore_index=True)\n",
                "\n",
                "print(disney_main_df.head)\n",
                "print(disney_main_df.shape)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "duplicates = disney_main_df[disney_main_df.duplicated(keep=False)]\n",
                "print(duplicates)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "missing_data = disney_main_df.isnull().sum()\n",
                "\n",
                "#printing missingdata in each column\n",
                "# Already know these numbers wil be very high from EDA as the majority of the data is string or object datatypes and or needs formatting updates as part of the cleaning process. \n",
                "print(missing_data[missing_data>0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check the shape (number of rows) of each DataFrame\n",
                "print(\"movies_total_gross shape:\", movies_total_gross.shape[0])\n",
                "print(\"disney_revenue shape:\", disney_revenue.shape[0])\n",
                "print(\"characters shape:\", characters.shape[0])\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "disney_main_df['total_gross'] = (\n",
                "    disney_main_df['total_gross']\n",
                "    .str.replace('$','',regex=False)\n",
                "    .str.replace(',','',regex=False)\n",
                "    .astype(float)\n",
                ")\n",
                "\n",
                "print(disney_main_df['total_gross'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#convert object data type to string\n",
                "\n",
                "disney_main_df['inflation_adjusted_gross'] = disney_main_df['inflation_adjusted_gross'].astype(str)\n",
                "\n",
                "#strip dollarsigns and commas to make data more uniform\n",
                "disney_main_df['inflation_adjusted_gross'] = (\n",
                "    disney_main_df['inflation_adjusted_gross']\n",
                "    .str.replace('$','',regex=False)\n",
                "    .str.replace(',','',regex=False)\n",
                "    .astype(float)\n",
                ")\n",
                "\n",
                "# convert the float datatype to numeric\n",
                "\n",
                "disney_main_df['inflation_adjusted_gross']=pd.to_numeric(disney_main_df['inflation_adjusted_gross'],errors='coerce')\n",
                "# Set display options for pandas\n",
                "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
                "print(disney_main_df['inflation_adjusted_gross'])\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Select rows 579 to 661\n",
                "rows_to_clean = disney_main_df.iloc[579:661]\n",
                "\n",
                "# Drop NaN values from the specified columns\n",
                "cleaned_rows = rows_to_clean.dropna(subset=[\n",
                "    'movie_title', 'release_date', 'genre', 'MPAA_rating',\n",
                "    'total_gross', 'inflation_adjusted_gross', 'Year', \n",
                "    'Studio Entertainment[NI 1]'\n",
                "])\n",
                "\n",
                "# Display the cleaned DataFrame\n",
                "print(cleaned_rows.to_string(index=False))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#remove NaN Values from rows 570 -661 as those values do not exist in the speciified columns since the csv. are different lengths and contaion different data\n",
                "# Select rows 579 to 661\n",
                "rows_to_clean = disney_main_df.iloc[579:661]\n",
                "\n",
                "# Check the original number of rows\n",
                "original_row_count = rows_to_clean.shape[0]\n",
                "print(f\"Original row count: {original_row_count}\")\n",
                "\n",
                "# Drop NaN values from the specified columns\n",
                "cleaned_rows = rows_to_clean.dropna(subset=[\n",
                "    'movie_title', 'release_date', 'genre', 'MPAA_rating',\n",
                "    'total_gross', 'inflation_adjusted_gross', 'Year', \n",
                "    'Studio Entertainment[NI 1]'\n",
                "])\n",
                "\n",
                "# Check the new number of rows\n",
                "new_row_count = cleaned_rows.shape[0]\n",
                "print(f\"New row count after dropping NaNs: {new_row_count}\")\n",
                "\n",
                "# Check for remaining NaN values in the cleaned DataFrame\n",
                "nan_counts = cleaned_rows.isna().sum()\n",
                "print(\"\\nRemaining NaN values in cleaned DataFrame:\")\n",
                "print(nan_counts)\n",
                "\n",
                "# Display the cleaned DataFrame\n",
                "print(\"\\nCleaned DataFrame:\")\n",
                "print(cleaned_rows.to_string(index=False))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#check column which contains nan values up to 579 but has daa after that\n",
                "print(disney_main_df['hero'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "1233f543-e9a0-4f78-96f5-d7536554102e"
            },
            "source": [
                "## Irregular Data\n",
                "\n",
                "Detect outliers in your dataset and handle them as needed. Use code comments to make notes about your thought process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "# Analyze rows 0-580 as that will contatin the bulk of the numerical data. \n",
                "rows_to_analyze = disney_main_df.iloc[0:580]\n",
                "\n",
                "# describe to see basic statitsics\n",
                "print(rows_to_analyze.describe())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "efed50ae-16f0-471d-98e2-632553a74c12"
            },
            "outputs": [],
            "source": [
                "\n",
                "#load dat frmt he above desribed stats\n",
                "data = {\n",
                "    'total_gross': [0, 12788864, 30702446, 75709033, 936662225],\n",
                "    'inflation_adjusted_gross': [0, 22741232, 55159783, 119202000, 5228953251]\n",
                "}\n",
                "\n",
                "disney_main_df = pd.DataFrame(data)\n",
                "\n",
                "# Calculate IQR for 'total_gross'\n",
                "Q1_gross = disney_main_df['total_gross'].quantile(0.25)\n",
                "Q3_gross = disney_main_df['total_gross'].quantile(0.75)\n",
                "IQR_gross = Q3_gross - Q1_gross\n",
                "\n",
                "# Outlier bounds for 'total_gross'\n",
                "lower_bound_gross = Q1_gross - 1.5 * IQR_gross\n",
                "upper_bound_gross = Q3_gross + 1.5 * IQR_gross\n",
                "\n",
                "# outliers in 'total_gross'\n",
                "outliers_gross = disney_main_df[(disney_main_df['total_gross'] < lower_bound_gross) | (disney_main_df['total_gross'] > upper_bound_gross)]\n",
                "print(\"Outliers in 'total_gross':\")\n",
                "print(outliers_gross)\n",
                "\n",
                "# Calculate IQR for 'inflation_adjusted_gross'\n",
                "Q1_adjusted = disney_main_df['inflation_adjusted_gross'].quantile(0.25)\n",
                "Q3_adjusted = disney_main_df['inflation_adjusted_gross'].quantile(0.75)\n",
                "IQR_adjusted = Q3_adjusted - Q1_adjusted\n",
                "\n",
                "# Outlier bounds for 'inflation_adjusted_gross'\n",
                "lower_bound_adjusted = Q1_adjusted - 1.5 * IQR_adjusted\n",
                "upper_bound_adjusted = Q3_adjusted + 1.5 * IQR_adjusted\n",
                "\n",
                "# outliers in 'inflation_adjusted_gross'\n",
                "outliers_adjusted = disney_main_df[(disney_main_df['inflation_adjusted_gross'] < lower_bound_adjusted) | (disney_main_df['inflation_adjusted_gross'] > upper_bound_adjusted)]\n",
                "print(\"Outliers in 'inflation_adjusted_gross':\")\n",
                "print(outliers_adjusted)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "6f5b8ee0-bab3-44bc-958a-67d1e4c0407f"
            },
            "source": [
                "## Unnecessary Data\n",
                "\n",
                "Look for the different types of unnecessary data in your dataset and address it as needed. Make sure to use code comments to illustrate your thought process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "e788a239-2fbf-41de-9bd3-19e52e3b187c"
            },
            "outputs": [],
            "source": [
                "# Not using the disney-director or disney-voice-actor CSVs as I have determined that information will not be relevant to this specifiv business question"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "53e0cf94-c68a-4fa0-9849-9505a66bcce6"
            },
            "source": [
                "## Inconsistent Data\n",
                "\n",
                "Check for inconsistent data and address any that arises. As always, use code comments to illustrate your thought process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "e9de6624-812a-43f8-8e20-93b4a49b091f"
            },
            "outputs": [],
            "source": [
                "\n",
                "movies_total_gross = pd.read_csv('disney_movies_total_gross.csv')\n",
                "disney_revenue = pd.read_csv('disney_revenue_1991-2016.csv')\n",
                "characters = pd.read_csv('Disney_Characters_cleaned.csv')\n",
                "\n",
                "disney_main_df = pd.concat([movies_total_gross,disney_revenue,characters],ignore_index=True)\n",
                "\n",
                "# object columns to convert to strings\n",
                "object_columns = [\n",
                "    'movie_title', 'release_date', 'genre', 'MPAA_rating', \n",
                "    'total_gross', 'inflation_adjusted_gross', 'Disney Media Networks', \n",
                "    'hero', 'villian', 'song', \n",
                "]\n",
                "\n",
                "# Convert object columns to strings and handle NaN values\n",
                "for col in object_columns:\n",
                "    disney_main_df[col] = disney_main_df[col].astype(str).replace('nan', '').str.strip()\n",
                "\n",
                "# Fill missing values with 'Unknown' for genre and 'Not Rated' for MPAA_rating\n",
                "disney_main_df['genre'] = disney_main_df['genre'].fillna('Unknown')\n",
                "disney_main_df['MPAA_rating'] = disney_main_df['MPAA_rating'].fillna('Not Rated')\n",
                "\n",
                "\n",
                "duplicates = disney_main_df.duplicated().sum()\n",
                "print(f\"Number of duplicate rows: {duplicates}\")\n",
                "\n",
                "# Check for unique values in categorical columns\n",
                "for col in ['genre', 'MPAA_rating']:\n",
                "    print(f\"Unique values in {col}: {disney_main_df[col].unique()}\")\n",
                "\n",
                "print(disney_main_df.info())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View unique values in the columns\n",
                "print(disney_main_df['total_gross'].unique())\n",
                "print(disney_main_df['inflation_adjusted_gross'].unique())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Replace empty strings with NaN\n",
                "disney_main_df['total_gross'] = disney_main_df['total_gross'].replace('', np.nan)\n",
                "disney_main_df['inflation_adjusted_gross'] = disney_main_df['inflation_adjusted_gross'].replace('', np.nan)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert to float after cleaning up the strings\n",
                "disney_main_df['total_gross'] = disney_main_df['total_gross'].replace({'$': '', ',': ''}, regex=True).astype(float)\n",
                "disney_main_df['inflation_adjusted_gross'] = disney_main_df['inflation_adjusted_gross'].replace({'$': '', ',': ''}, regex=True).astype(float)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fill NaN values with 0\n",
                "disney_main_df['total_gross'] = disney_main_df['total_gross'].fillna(0)\n",
                "disney_main_df['inflation_adjusted_gross'] = disney_main_df['inflation_adjusted_gross'].fillna(0)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "dedc0bfe-17d0-40b2-914f-2ddb54f9ce0d"
            },
            "source": [
                "## Summarize Your Results\n",
                "\n",
                "Make note of your answers to the following questions.\n",
                "\n",
                "1. Did you find all four types of dirty data in your dataset?\n",
                "    No I found 3 of 4 there was no duplicate data that I could find.\n",
                "2. Did the process of cleaning your data give you new insights into your dataset?\n",
                "    Yes it helped me to elimnate to CSV's that were not useful and brought down the toal row count. It also showed me the structure in a more understanabdle way allowing e to see why there were so many rows in the first place. \n",
                "3. Is there anything you would like to make note of when it comes to manipulating the data and making visualizations?\n",
                "    I will defintiely be considering total gross nad adjusted gross in comparison to year of release and character in film to help make recommendations for merchandise and characters. "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
